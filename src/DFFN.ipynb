{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "DFFN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HxhetsfSiP9",
        "colab_type": "text"
      },
      "source": [
        "**ΠΑΡΑΚΑΤΩ ΕΧΟΥΜΕ ΑΠΛΑ ΠΑΡΑΔΕΙΓΜΑΤΑ ΝΕΥΡΩΝΙΚΩΝ ΔΙΚΤΥΩΝ ΜΕ ΤΗΝ ΧΡΗΣΗ ΔΙΑΦΟΡΩΝ ΤΕΧΝΙΚΩΝ ΟΜΑΛΟΠΟΙΗΣΗΣ\n",
        "ΣΥΜΒΟΥΛΗ ΜΟΥ ΕΙΝΑΙ ΟΤΙ Ο L2 REG ΘΑ ΟΔΗΓΗΣΕΙ ΣΕ ΚΑΛΥΤΕΡΑ ΑΠΟΤΕΛΕΣΜΑΤΑ ΚΑΙ ΕΠΙΣΗΣ ΓΙΑ ΤΟ ΠΟΣΟ ΒΑΘΥ ΜΠΟΡΕΙ ΝΑ ΕΙΝΑΙ ΕΝΑ ΔΙΚΤΥΟ \n",
        "ΚΑΘΩς ΚΑΙ ΤΟ ΠΟΣΟ ΠΥΚΝΟ ΝΑ ΧΡΗΣΙΜΟΠΟΙΗΣΕΙΣ ΤΗΝ ΣΥΝΑΡΤΗΣΗ GRID\n**"
         ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTjrxq5W962q",
        "colab_type": "code",
        "outputId": "b12316bf-836e-457e-b8bd-64df03c1d0a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install --upgrade keras\n",
        "!pip install --upgrade matplotlib\n",
        "!pip install --upgrade numpy\n",
        "!pip install --upgrade tensorflow\n",
        "!pip install --upgrade pandas\n",
        "!pip install --upgrade pactools"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (46.0.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
            "Requirement already up-to-date: numpy in /usr/local/lib/python3.6/dist-packages (1.18.2)\n",
            "Requirement already up-to-date: tensorflow in /usr/local/lib/python3.6/dist-packages (2.2.0rc1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard<2.2.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.2)\n",
            "Requirement already satisfied, skipping upgrade: tensorflow-estimator<2.3.0,>=2.2.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.2.0rc0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.2.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (46.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (1.7.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow) (0.4.8)\n",
            "Requirement already up-to-date: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already up-to-date: pactools in /usr/local/lib/python3.6/dist-packages (0.2.0b0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8jjrH3-9622",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65140a41-0551-49f2-82c9-238cda9df802"
      },
      "source": [
        "#ΧΡΗΣΙΜΕΣ ΒΙΒΛΙΟΘΉΚΕΣ \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "#TESNORFOW\n",
        "import tensorflow as tf\n",
        "\n",
        "#KERAS LIBRARIES\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout , Flatten\n",
        "from keras.regularizers import l1, l2, l1_l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras import backend\n",
        "\n",
        "#CROSS VALIDATION\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from pactools.grid_search import GridSearchCVProgressBar\n",
        "\n",
        "#WRAPPERS NEEDED FOR GRIDSEARCH\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.wrappers.scikit_learn import KerasRegressor"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxue1O5-7fs8",
        "colab_type": "text"
      },
      "source": [
        "#EXAMPLE1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DmIEYkh963A",
        "colab_type": "code",
        "outputId": "207afa3c-039f-4c12-dca0-eb23da03efe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "fashion_mnist = keras.datasets.fashion_mnist  # load dataset\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DI9znL6963G",
        "colab_type": "code",
        "outputId": "d0e38a4b-68cc-481e-9530-b38f470079cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(\"train_images SHAPE :\", train_images.shape)\n",
        "print(\"train_labels SHAPE :\", train_labels.shape)\n",
        "print(\"test_images SHAPE :\", test_images.shape)\n",
        "print(\"test_labels SHAPE :\", test_labels.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_images SHAPE : (60000, 28, 28)\n",
            "train_labels SHAPE : (60000,)\n",
            "test_images SHAPE : (10000, 28, 28)\n",
            "test_labels SHAPE : (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnEMxdgf963N",
        "colab_type": "code",
        "outputId": "88d2d425-3d0a-483c-e200-64e5e39079af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_labels1=pd.DataFrame(data=train_labels,columns=['target'])\n",
        "np.sort(train_labels1.target.unique())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIpcwEPP963U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgHt9mSr963h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Smaller values will make easier for our model to process\n",
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0\n",
        "batch_size=5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_OGzDM6963p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model128(): #MODEL WITH 1 HIDDEN LAYER , 128 NEURONS\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),  # input layer (1)\n",
        "        Dense(128, activation='relu'),  # hidden layer (2)\n",
        "        Dense(10, activation='softmax') # output layer (3)\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def model33(): #MODEL WITH 1 HIDDEN LAYER, 33 NEURONS\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),  # input layer (1)\n",
        "        Dense(33, activation='relu'),  # hidden layer (2)\n",
        "        Dense(10, activation='softmax') # output layer (3)\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def model340(): #MODEL WITH 1 HIDDEN LAYER, 340 NEURONS\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),  # input layer (1)\n",
        "        Dense(340, activation='relu'),  # hidden layer (2)\n",
        "        Dense(10, activation='softmax') # output layer (3)\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def model523(): # MODEL WITH 1 HIDDEN LAYER , 523 NEURONS\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(28, 28)),  # input layer (1)\n",
        "        Dense(523, activation='relu'),  # hidden layer (2)\n",
        "        Dense(10, activation='softmax') # output layer (3)\n",
        "    ])\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdjBp5NQ963x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare(train_images,train_labels,epochs,test_images,test_labels):\n",
        "    model1 = model128()\n",
        "    model2 = model33()\n",
        "    model3 = model340()\n",
        "    model4 = model523()\n",
        "    \n",
        "    model1.fit(train_images, train_labels, epochs=epochs,verbose=0,batch_size=batch_size)\n",
        "    model2.fit(train_images, train_labels, epochs=epochs,verbose=0,batch_size=batch_size)\n",
        "    model3.fit(train_images, train_labels, epochs=epochs,verbose=0,batch_size=batch_size)\n",
        "    model4.fit(train_images, train_labels, epochs=epochs,verbose=0,batch_size=batch_size)\n",
        "    \n",
        "    test_loss1, test_acc1 = model1.evaluate(test_images,  test_labels, verbose=2)\n",
        "    test_loss2, test_acc2 = model2.evaluate(test_images,  test_labels, verbose=2)\n",
        "    test_loss3, test_acc3 = model3.evaluate(test_images,  test_labels, verbose=2)\n",
        "    test_loss4, test_acc4 = model4.evaluate(test_images,  test_labels, verbose=2)\n",
        "    \n",
        "    print('Test accuracy for 128 neurons and ',epochs,' epochs:', test_acc1)\n",
        "    print('Test accuracy for 33 neurons and ',epochs,' epochs:', test_acc2)\n",
        "    print('Test accuracy for 340 neurons and ',epochs,' epochs:', test_acc3)\n",
        "    print('Test accuracy for 523 neurons and ',epochs,' epochs:', test_acc4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2IRTjnAG9633",
        "colab_type": "code",
        "outputId": "99849136-2aab-43be-df2d-411f9e678ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "compare(train_images,train_labels,50,test_images,test_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy for 128 neurons and  50  epochs: 0.8716999888420105\n",
            "Test accuracy for 33 neurons and  50  epochs: 0.8555999994277954\n",
            "Test accuracy for 340 neurons and  50  epochs: 0.8792999982833862\n",
            "Test accuracy for 523 neurons and  50  epochs: 0.8809000253677368\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HtBw98E3oj8T",
        "colab_type": "text"
      },
      "source": [
        "#EXAMPLE2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElG8jgTv964G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "number_of_features=1000\n",
        "epochs=50\n",
        "batch_size=1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBHhG_vr964M",
        "colab_type": "code",
        "outputId": "dc5282f0-2683-4db9-8b65-1dd6a1ccc0ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "(X_train_data, y_train), (X_test_data, y_test) = imdb.load_data(\n",
        "    num_words=number_of_features)\n",
        "X1=np.concatenate((X_train_data, X_test_data), axis=0)\n",
        "Y=np.concatenate((y_train, y_test), axis=0)\n",
        "print('Σύνολο Εκπαίδευσης')\n",
        "print('------------------')\n",
        "print('Μέγεθος Δεδομένων: {}'.format(X_train_data.shape))\n",
        "print('Μέγεθος Ετικετών: {}'.format(y_train.shape))\n",
        "print('')\n",
        "print('Σύνολο Ελέγχου')\n",
        "print('------------------')\n",
        "print('Μέγεθος Δεδομένων: {}'.format(X_test_data.shape))\n",
        "print('Μέγεθος Ετικετών: {}'.format(y_test.shape))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Σύνολο Εκπαίδευσης\n",
            "------------------\n",
            "Μέγεθος Δεδομένων: (25000,)\n",
            "Μέγεθος Ετικετών: (25000,)\n",
            "\n",
            "Σύνολο Ελέγχου\n",
            "------------------\n",
            "Μέγεθος Δεδομένων: (25000,)\n",
            "Μέγεθος Ετικετών: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGY9Vp_E964X",
        "colab_type": "code",
        "outputId": "8c608373-ef07-4443-bfe0-45d4855e7e38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "print('Σύνολο Εκπαίδευσης')\n",
        "print('------------------')\n",
        "print('Δεδομένο: {}'.format(X_train_data[0]))\n",
        "print('Ετικέτα: {}'.format(y_train[0]))\n",
        "print('')\n",
        "print('Σύνολο Ελέγχου')\n",
        "print('------------------')\n",
        "print('Δεδομένο: {}'.format(X_test_data[0]))\n",
        "print('Ετικέτα: {}'.format(y_test[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Σύνολο Εκπαίδευσης\n",
            "------------------\n",
            "Δεδομένο: [1, 14, 22, 16, 43, 530, 973, 2, 2, 65, 458, 2, 66, 2, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 2, 2, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2, 19, 14, 22, 4, 2, 2, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 2, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2, 2, 16, 480, 66, 2, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 2, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 2, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 2, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "Ετικέτα: 1\n",
            "\n",
            "Σύνολο Ελέγχου\n",
            "------------------\n",
            "Δεδομένο: [1, 591, 202, 14, 31, 6, 717, 10, 10, 2, 2, 5, 4, 360, 7, 4, 177, 2, 394, 354, 4, 123, 9, 2, 2, 2, 10, 10, 13, 92, 124, 89, 488, 2, 100, 28, 2, 14, 31, 23, 27, 2, 29, 220, 468, 8, 124, 14, 286, 170, 8, 157, 46, 5, 27, 239, 16, 179, 2, 38, 32, 25, 2, 451, 202, 14, 6, 717]\n",
            "Ετικέτα: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFiRDOI9964c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(num_words=number_of_features)\n",
        "\n",
        "X_train_features = tokenizer.sequences_to_matrix(X_train_data, mode='count')\n",
        "X_test_features = tokenizer.sequences_to_matrix(X_test_data, mode='count')\n",
        "X = tokenizer.sequences_to_matrix(X1, mode='count')\n",
        "\n",
        "Y_train_soft = backend.one_hot(y_train,2).numpy()\n",
        "Y_test_soft = backend.one_hot(y_test,2).numpy()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D2Nr2wcy964h",
        "colab_type": "code",
        "outputId": "d03fbfc2-c617-4cd9-d88e-7bcfcf3fd4f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Μέγεθος Δεδομένων Εκπαίδευσης: {}'.format(X_train_features.shape))\n",
        "print('Μέγεθος Δεδομένων Ελέγχου: {}'.format(X_test_features.shape))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Μέγεθος Δεδομένων Εκπαίδευσης: (25000, 1000)\n",
            "Μέγεθος Δεδομένων Ελέγχου: (25000, 1000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN5cQDsJ964o",
        "colab_type": "code",
        "outputId": "ec249501-1e41-4abb-d049-177f016739c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('Πρώτο δεδομένο εκπαίδευσης:\\n {}'.format(X_train_features[0]))\n",
        "print('\\nΠρώτο δεδομένο ελέγχου:\\n {}'.format(X_test_features[0]))\n",
        "print('\\nΠρώτο αποτέλεσμα εκπαιδευσης:\\n {}'.format(Y_train_soft[0]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Πρώτο δεδομένο εκπαίδευσης:\n",
            " [ 0.  1. 29.  0. 15.  9.  3.  2.  3.  1.  0.  0.  6.  3.  3.  4. 11.  3.\n",
            "  3.  2.  0.  1.  6.  0.  0.  4.  3.  0.  2.  0.  1.  0.  3.  2.  0.  1.\n",
            "  4.  0.  4.  1.  0.  0.  0.  4.  0.  0.  1.  0.  1.  0.  2.  2.  1.  0.\n",
            "  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  0.  2.  2.  0.  0.  0.  0.  2.\n",
            "  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  2.  0.\n",
            "  0.  0.  1.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  1.  2.  0.  1.  1.\n",
            "  0.  0.  0.  0.  2.  1.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.\n",
            "  0.  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  1.  0.  0.\n",
            "  1.  0.  0.  1.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  2.  1.  0.  0.  0.  0.  1.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  1.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  2.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  1.  0.  0.  0.  1.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  1.  0.  0.  0.  0.  0.  0.  2.  0.  0.  0.  3.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  3.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            "\n",
            "Πρώτο δεδομένο ελέγχου:\n",
            " [ 0.  1. 11.  0.  3.  2.  2.  1.  2.  1.  4.  0.  0.  1.  4.  0.  1.  0.\n",
            "  0.  0.  0.  0.  0.  1.  0.  1.  0.  2.  1.  1.  0.  2.  1.  0.  0.  0.\n",
            "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.\n",
            "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  2.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  1.  0.  1.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  2.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
            "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  2.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
            "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
            "\n",
            "Πρώτο αποτέλεσμα εκπαιδευσης:\n",
            " [0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5BQdx6h965A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pff(h_base,titlos): #Plot For Fitting-Συνάρτηση για να βλέπουμε το capacity δηλαδή το overfitting - underfitting\n",
        "  ax = plt.figure().gca()\n",
        "  ax.plot(h_base.epoch, h_base.history['loss'], h_base.epoch, h_base.history['val_loss'])\n",
        "\n",
        "  ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
        "\n",
        "  plt.title(titlos)\n",
        "  plt.ylabel('Αντικειμενική Συνάρτηση')\n",
        "  plt.xlabel('Εποχή')\n",
        "  plt.legend(loc=1, labels=(\"Train Error\", 'Test Error'))\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHjULRVc964u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def basic(X_train_features, y_train,X_test_features, y_test): #Basic NN with 2 Hidden layers\n",
        "  base = Sequential([\n",
        "      Dense(32, activation='relu', input_shape=(number_of_features,)),\n",
        "      Dense(16, activation='relu'),\n",
        "      Dense(2, activation='softmax')  ##!!!!!!!!!!!! OTAN XRHSIMOPOEIS SOFTMAX PREPEI TA LABEL SOU NA EINAI TOSA OSA KAI TO OUTPUT, AN XREIASTEI KANE ONE HOT!!!!!\n",
        "  ])\n",
        "\n",
        "  base.compile(loss='categorical_crossentropy', optimizer='adadelta', \n",
        "              metrics=['accuracy'])\n",
        "  \n",
        "  '''# GIA TO APLO MONTELO\n",
        "  return base\n",
        "  '''\n",
        "  '''\n",
        "  # GIA EARLY STOPPING \n",
        "  call = early('simple',2)\n",
        "  h_base = base.fit(X_train_features, y_train, epochs=epochs, verbose=0, \n",
        "                    batch_size=batch_size, \n",
        "                    callbacks=call,\n",
        "                    validation_data=(X_test_features, y_test))\n",
        "   \n",
        "  test_loss, test_acc = base.evaluate(X_test_features, y_test, verbose=0)\n",
        "  \n",
        "  print(\"BASIC WITH EARLY STOPPING:\", test_acc)\n",
        "  return h_base\n",
        "  #best_model = load_model('simple')\n",
        "  #return best_model\n",
        "\n",
        "  '''\n",
        "  \n",
        "  #TRAIN SESSION\n",
        "  h_base = base.fit(X_train_features, y_train, epochs=epochs, verbose=0, \n",
        "                    batch_size=batch_size, \n",
        "                    validation_data=(X_test_features, y_test))\n",
        "  \n",
        "  test_loss, test_acc = base.evaluate(X_test_features, y_test, verbose=0)\n",
        "  \n",
        "  print(\"BASIC:\", test_acc)\n",
        "  return h_base\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def regl1(X_train_features, y_train,X_test_features, y_test1,param): ## NN with 2 hidden layers and L1 regularization\n",
        "    d_l1 = Sequential([\n",
        "        Dense(64, activation='relu',input_shape=(number_of_features,), kernel_regularizer=l1(param)),\n",
        "        Dense(32, activation='relu', kernel_regularizer=l1(param)),\n",
        "        Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    d_l1.compile(loss='binary_crossentropy', optimizer='rmsprop', \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    #return d_l1\n",
        "    \n",
        "    #TRAIN SESSION\n",
        "    h_l1 = d_l1.fit(X_train_features, y_train, epochs=epochs, verbose=0, \n",
        "                    batch_size=batch_size, \n",
        "                    validation_data=(X_test_features, y_test))\n",
        "\n",
        "    test_loss, test_acc = d_l1.evaluate(X_test_features, y_test, verbose=0)\n",
        "    \n",
        "    print(\"L1 REGULARIZATION ACCURACY:\", test_acc)\n",
        "    return h_l1\n",
        "    \n",
        "\n",
        "def regl2(X_train_features, y_train,X_test_features, y_test1,param): ## NN with 2 hidden layers and L2 regularization\n",
        "  d_l2 = Sequential([\n",
        "    Dense(units=64, activation='relu', kernel_regularizer=l2(param)),\n",
        "    Dense(units=32, activation='relu', kernel_regularizer=l2(param)),\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "  ])\n",
        "  \n",
        "  d_l2.compile(loss='binary_crossentropy', optimizer='rmsprop', \n",
        "             metrics=['accuracy'])\n",
        "  \n",
        "  #return d_l2\n",
        "  \n",
        "  # TRAIN SESSION \n",
        "  h_l2 = d_l2.fit(X_train_features, y_train, epochs=epochs, verbose=0, \n",
        "                batch_size=batch_size, \n",
        "                validation_data=(X_test_features, y_test))\n",
        "\n",
        "  test_loss, test_acc = d_l2.evaluate(X_test_features, y_test, verbose=0)\n",
        "    \n",
        "  print(\"L2 REGULARIZATION ACCURACY:\", test_acc)\n",
        "  return h_l2\n",
        "  \n",
        "\n",
        "def drop(X_train_features, y_train,X_test_features, y_test1): ## NN with 2 hidden layers and L2 regularization\n",
        "  \n",
        "  d_hd = Sequential([\n",
        "    Dropout(0.1, input_shape=(number_of_features,)),\n",
        "    Dense(units=64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(units=32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "  \n",
        "  d_hd.compile(loss='binary_crossentropy', optimizer='rmsprop', \n",
        "             metrics=['accuracy'])\n",
        "  \n",
        "  #return d_hd\n",
        "  \n",
        "  #TRAIN SESSION\n",
        "  h_hd = d_hd.fit(X_train_features, y_train, epochs=epochs, verbose=0, \n",
        "                batch_size=batch_size, \n",
        "                validation_data=(X_test_features, y_test))\n",
        "\n",
        "  test_loss, test_acc = d_hd.evaluate(X_test_features, y_test, verbose=0)\n",
        "    \n",
        "  print(\"DROPOUT MODEL ACCURACY:\", test_acc)\n",
        "  return h_hd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1Eqn_1rmUNa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(dense_layer_sizes, optimizer=\"adam\", dropout=0.1, init='uniform', nbr_features=1000, dense_nparams=256):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(dense_nparams, activation='relu', input_shape=(nbr_features,), kernel_initializer=init,)) \n",
        "    model.add(Dropout(dropout), )\n",
        "    for layer_size in dense_layer_sizes:\n",
        "        model.add(Dense(layer_size, activation='relu'))\n",
        "        model.add(Dropout(dropout), )\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVy3uaGQn7qN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "a020cf47-300a-429a-df03-9b721e456988"
      },
      "source": [
        "m=create_model((10,10))\n",
        "m.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 256)               256256    \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 10)                2570      \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 258,947\n",
            "Trainable params: 258,947\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OavpnWTLONW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def early(name,rounds):\n",
        "  callbacks = [EarlyStopping(monitor='val_loss', patience=rounds),\n",
        "              ModelCheckpoint(\n",
        "                  filepath=name, \n",
        "                  monitor='val_loss', \n",
        "                  save_best_only=True)\n",
        "              ]\n",
        "  return callbacks\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc8dC7OtGpop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compareReg():# 50 epochs !! Compare Regularization Algorithms , comment out to exclude model \n",
        "  #basicmodel=basic(X_train_features, Y_train_soft,X_test_features, Y_test_soft)\n",
        "  #l1model=regl1(X_train_features, y_train ,X_test_features, y_test,0.001)\n",
        "  l2model=regl2(X_train_features, y_train ,X_test_features, y_test, 0.1)\n",
        "  #dropoutmodel=drop(X_train_features, y_train ,X_test_features, y_test)\n",
        "  #pff(basicmodel,'BASIC')\n",
        "  #pff(l1model,'L1 REG')\n",
        "  pff(l2model,'L2 REG')\n",
        "  #pff(dropoutmodel,'DROPOUT')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHQRabpt3sD7",
        "colab_type": "code",
        "outputId": "31df0a56-f70d-43fa-f8e7-af4d2c3f8455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        }
      },
      "source": [
        "compareReg()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L2 REGULARIZATION ACCURACY: 0.8231599926948547\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAEXCAYAAABF40RQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZxcZZ3v8c+vlq7q7uokJOmsnR0I\nWyCBFhAXCAhuiIxXBFFBULm4gKAOil4FZ+SOy4jCxdHBGcAFIbLLooiQCBk2Q1gChCWBEAJZOnvv\n1VX1u3+c6k6ns1W6+1R1d33fr9d51dnqnF91Kr/z1HOe8zzm7oiISHmJlDoAEREpPiV/EZEypOQv\nIlKGlPxFRMqQkr+ISBlS8hcRKUNK/iIiZUjJX8qOma0ws/ftZP3RZvaAmW00swYzu8XMxu/mOAvM\nrM3MmsxsvZnd3n1/M7vczDry2zunzd22m5l9xcyeM7MWM1uTP+YZ/f+pRban5C+yzT7AtcBUYArQ\nCFy/h/d8xd1TwL5ACvj3HtvnuXuq2zSi27argYuArwOjgInA/wE+0NcPIrInsVIHIDJQuPufuy+b\n2TXA3wt872YzuxP4ciH7m9n+wJeAo9x9UbdNC/OTSKhU8hfZtfcCLxSyo5mNAj4GLCvw2McDb/ZI\n/CJFo+QvshNmdijwPeCf97Dr1Wa2BVgPjAYu6LH9E2a2uds0P79+NLCmxzlX5fdpM7Mp/fAxRHZJ\nyV+kBzPbF/gz8FV3f2QPu1/o7sOBQwnuGdT12P5Hdx/RbZqbX78B2O5msrvXEVwUEoD19XOI7I6S\nv0g3+RL334B/dfffFfo+d18C/AD4hZkVkrgfAurMrL53kYr0jZK/lKu4mSW7TTEzm0iQlK9x91/1\n4pi/AcYCp+xpR3d/GfhP4GYzO9HMKs0sChzTi/OK7DUlfylX9wGt3abLgc8D04HLu7fNL/SA7p4G\nrgK+22316T3a+TeZ2Zj8ti8TNPe8EtgIrAL+FTgdWNmnTyeyB6bBXEREyo9K/iIiZUjJX0SkDCn5\ni4iUISV/EZEyNGj69hk9erRPnTq11GGIiAwqTz311Hp3r+25ftAk/6lTp7JokbpBERHZG2b2xs7W\nq9pHRKQMKfmLiJQhJX8RkTI0aOr8RWTw6+joYNWqVbS1tZU6lCEnmUxSV1dHPB4vaH8lfxEpmlWr\nVlFTU8PUqVMprPNTKYS7s2HDBlatWsW0adMKeo+qfUSkaNra2hg1apQSfz8zM0aNGrVXv6iU/EWk\nqJT4w7G3f9chn/zveHoVv398p81cRUTKVqjJ38yuM7N1ZvZ8t3UjzewBM3s1/7pPmDHc+9xqbnpS\nXaOLCGzYsIHZs2cze/Zsxo0bx8SJE7uW0+n0bt+7aNEiLrzwwr0639SpU5k1a1bXOfb2/WEK+4bv\nDcA1wG+7rfsW8KC7/9DMvpVf/mZYAaQSMZraM2EdXkQGkVGjRvHMM88AcPnll5NKpfjGN77RtT2T\nyRCL7Twt1tfXU1+/96Nuzp8/n9GjR+9ye89z7i6G7rLZLNFodK/j6RRqyd/dHyYYoai7jxIMd0f+\n9dQwY0glYzS1KfmLyM599rOf5fzzz+eoo47ikksu4cknn+Sd73wnc+bM4ZhjjuHll18GYMGCBZx8\n8slAcOE499xzOe6445g+fTpXX331Xp3zuOOO46KLLqK+vp6rrrpqh+UHH3yQOXPmMGvWLM4991za\n29uB4JfEN7/5TQ4//HBuueWWPn3uUjT1HOvuq/PzawjGPA1NKhGnUclfZMD5/t0v8OLbW/v1mAdN\nGMZlHzl4r9+3atUqHn30UaLRKFu3buWRRx4hFovxt7/9jW9/+9vcdtttO7znpZdeYv78+TQ2NjJz\n5ky++MUv7rSN/dy5c7tK6GeffTYXX3wxAOl0uqu/srvvvrtrua2tjf32248HH3yQ/fffn7POOotf\n/vKXXHTRRUDw62Xx4sV7/Rl7Kmk7f3d3M9vlOJJmdh5wHsDkyZN7dY6aZIx0Nkd7Jksi1vufSCIy\ndJ122mldCXrLli2cffbZvPrqq5gZHR0dO33Phz/8YRKJBIlEgjFjxrB27Vrq6up22G9X1T6nn376\nTpdffvllpk2bxv777w8EF4xf/OIXXcm/5/t6qxTJf62ZjXf31WY2Hli3qx3d/VrgWoD6+vpeDTac\nSgQfsaktQyKl5C8yUPSmhB6W6urqrvnvfve7zJ07lzvuuIMVK1Zw3HHH7fQ9iUSiaz4ajZLJ7F0N\nQ/dz7my50Pf1Vimaev4JODs/fzZwV5gnq0nmk79u+opIAbZs2cLEiRMBuOGGG4p+/pkzZ7JixQqW\nLVsGwO9+9zuOPfbYfj9P2E09bwIeA2aa2Soz+xzwQ+BEM3sVeF9+OTSdJX/V+4tIIS655BIuvfRS\n5syZs9el+Z2ZO3duV1PPs846a4/7J5NJrr/+ek477TRmzZpFJBLh/PPP73McPZl7r2pTiq6+vt57\nM5jLo8vXc+avn+Dm847m6OmjQohMRAq1dOlSDjzwwFKHMWTt7O9rZk+5+w5tVIf8E741ieDuu5p7\niohsM+STf0p1/iIiOxj6yb+zzl/JX0Sky5BP/l2tfVTtIyLSZcgn/0QsQixiNLXv/EENEZFyNOST\nv5mpfx8RkR6GfPKHoN5f7fxFpC9dOkPQudujjz6602033HADtbW1XcebPXs2L774Yn9/hH5TFmP4\nphIx3fAVkT126bwnCxYsIJVKccwxx+x0++mnn84111yzy/f3tvvmQvfbG2VR8q9RtY+I7MJTTz3F\nscceyxFHHMH73/9+Vq8OOh2++uqrOeiggzj00EM544wzWLFiBb/61a/42c9+xuzZs3nkkUcKOv6C\nBQt4z3vewymnnMJBBx20w3JbWxvnnHMOs2bNYs6cOcyfPx8IfkmccsopHH/88Zxwwgn9/rnLpuS/\nvmnPP+lEpIj+/C1Ys6R/jzluFnyw8B5j3J0LLriAu+66i9raWubNm8d3vvMdrrvuOn74wx/y+uuv\nk0gk2Lx5MyNGjOD888/f7a+FefPmsXDhwq7lxx57DIDFixfz/PPPM23aNBYsWLDd8k9/+lPMjCVL\nlvDSSy9x0kkn8corr3S977nnnmPkyJF9+KPsXHkk/2ScFRtaSh2GiAww7e3tPP/885x44olAMDrW\n+PHjATj00EP51Kc+xamnnsqppxY25tSuqn2OPPJIpk2bttPlhQsXcsEFFwBwwAEHMGXKlK7kf+KJ\nJ4aS+KFMkn9NUjd8RQacvSihh8XdOfjgg7tK6N3de++9PPzww9x9991cccUVLFnS+18ppe6+eWfK\no84/EVM7fxHZQSKRoKGhoSv5d3R08MILL5DL5XjzzTeZO3cuP/rRj9iyZQtNTU3U1NTQ2NjYrzG8\n5z3v4cYbbwTglVdeYeXKlcycObNfz7EzZZH8U4kYbR05OrK5UociIgNIJBLh1ltv5Zvf/CaHHXYY\ns2fP5tFHHyWbzfLpT3+66ybshRdeyIgRI/jIRz7CHXfcscsbvvPmzduuqeeumoV296UvfYlcLses\nWbM4/fTTueGGG7YbKCYsQ75LZ4Dr/+d1vn/3izzzvRMZUVXRz5GJSKHUpXO41KVzDxrQRURke2WR\n/DWUo4jI9soi+ac6B3RR8hcpucFS1TzY7O3ftTySv7p1FhkQkskkGzZs0AWgn7k7GzZsIJlMFvye\nsmjn31nnv7VNzT1FSqmuro5Vq1bR0NBQ6lCGnGQySV1dXcH7l0XyV52/yMAQj8e3e9JVSqc8qn0S\nqvYREemuLJJ/VUUUM5X8RUQ6lUXyNzMN6CIi0k1ZJH+AYcm4Sv4iInllk/xTCQ3oIiLSqXySfzKm\nkr+ISF75JH+N4ysi0qWgdv5mNg74AjAZiPbc7u7n9nNc/S6VjLFqk0bzEhGBwh/yugt4CPgrMCg7\nxQ8GdFHJX0QECk/+SXe/NNRIQqYbviIi2xRa53+vmX0o1EhClkrGaE5nyebUoZSISKHJ/wLgHjNr\nNbOtZtZoZlvDDKy/dXXxoKofEZHCkr+717h7xN0r3X1YfnlYX05sZheb2Qtm9ryZ3WRmhfdF2gvq\n3E1EZJuCkr+ZvWhmo83so2b2YTMb3ZeTmtlE4EKg3t0PIWhBdEZfjrknXQO6qN5fRKTgap8DgH8A\nHyNI0ovM7GN9PHcMqDSzGFAFvN3H4+1W14Au7erTX0Sk0NY+W4Ej3H0jgJmNJWj2eXtvTurub5nZ\nvwMrgVbgr+7+1577mdl5wHkAkydP7s2pumgQdxGRbQot+b/dmfjzGgDr7UnNbB/go8A0YAJQbWaf\n7rmfu1/r7vXuXl9bW9vb0wEwTHX+IiJdCi35P2Fm9wJ/JLhgnEHw4FdvvQ943d0bAMzsduAY4Pd9\nOOZuaRxfEZFtCk3+nwM+CbwDyABXuft9fTjvSuBoM6siqPY5AVjUh+PtkZp6iohsU1Dyd/cccGN+\n6jN3f8LMbgUWE1xMngau7Y9j70p1her8RUQ6Fdqx2yeAHwI9m3ga4L1p8+/ulwGX7e37eisSCUbz\nUslfRKTwap9/Az7o7i+HGUzY1L+PiEig0NY+6wZ74ofgpm+j2vmLiBRc8v+Hmc0D7gTaO1e6e6/a\n+ZeKBnEXEQkUmvxrgBbgpG7rnF4+5FUqNRrKUUQEKDz5f8HdB33WTCVirNnSVuowRERKrtA6/yd7\nrjCzw/s5ltCptY+ISKBXA7ibWYIQn8YNSyqp1j4iIlB4tc+hPQZvaQR+FkI8oapJxGhKZ8jlnEik\n110TiYgMeoUm/yXuPifUSIqgJhnHHVo6sl3dPYiIlKNeVfsMVurcTUQkUGjyf2eoURTJts7d9KCX\niJS3QpP/ODO7y8wazGxdfn56qJGFoLPkrwe9RKTcFZr8/wDcCownGHzlFuCmsIIKS426dRYRAQpP\n/iPd/XfunslPvweSYQYWBtX5i4gECm3y8pqZXUpQ2nfgdOA+MxsJ0GOIxwFL4/iKiAQKTf5nAP9K\nMHTjsB7rHRgU9f81iTgAjar2EZEyV+hIXluBr4YcS+iqE1FA1T4iIgXV+ZvZ181s3x7rrg4npPDE\nohEq41E19RSRsrfL5G9mY7otNgAPm9lLZnaTmT0LDMruMVPq1llEZLfVPh8wsyOBi4BPENT3/4Wg\nX5/TgFHhh9f/ajSgi4jIrpO/u//WzN4iuKk7yd1P7rb5ITO7K/ToQqCSv4jIHm74uvuDAGb2oXwd\nf+cwju8CcuGH1/9q1K2ziEjBD3l9DngLuBD4Z6ACODusoMKkAV1ERApv6tlqZvcC/wCedPemcMMK\nTyoRV52/iJS9gpK/mc0nGMC9Hag3s8vc/fpQIwuJBnEXESn8Cd/PuPsqADMbC1xjZke6+xfDCy0c\nndU+7o6ZRvMSkfJUUJ1/t8R/OPB5gt49Px5iXP3nL5fCHduuUalkjGzOae3IljAoEZHSKvQJ39/l\nm31enX/P14Axu3/XANG4GlY92bXYNaCL6v1FpIwVWu1zJ/AVd98SZjChqBkPrz6wbbFzQJf2zCC5\neomI9L9Ck//9wIfNbBKwGviTuzeGF1Y/So2FdBO0N0KiRiV/EREKb+d/P3AgkAaOAZ4xs/eFFlV/\nqhkXvDauBbqP46vkLyLlq9CS/7Hu3pUtzWwCcB8wO5So+lNn8m9aA6P31Ti+IiIU3tqnZ6bMAlV9\nObGZjTCzW/M9hS41s3f25Xi7lOos+a8Btg3oopK/iJSzQh/yaiQYsavTOuCSPp77KuAv7v5xM6ug\njxeTXarZPvlvG8dXffqLSPkqtHuHmv48qZkNB94LfDZ//DTB/YT+lxwOsWTQ5BPV+YuIwB6qfczs\n3N1s60vJfxrBADHXm9nTZvZfZla9k3OcZ2aLzGxRQ0ND785kFpT+m4IbvhWxCIlYROP4ikhZ21Od\n/1c6Z8xsupl13/+MPpw3BhwO/NLd5wDNwLd67uTu17p7vbvX19bW9v5sqXFd1T6gbp1FRPaU/A3A\nzD4NLAQeN7MDu2/rpVXAKnd/Ir98K8HFIBw12yd/dessIuVuT3X+MTO7kqBJ52HA0cB9ZvafFP6M\nwA7cfY2ZvWlmM939ZeAE4MXeHm+PasbBsge7FlNJDeUoIuVtTwn8YCAKnOjuDe5+N1APzAEO6eO5\nLwBuNLPnCC4u/7ePx9u1mnGQboR0M5Av+Sv5i0gZ21PJf5K7v9V9hbtvAE43s7q+nNjdnyG4kISv\ne1v/UTNIJeK8tbm1KKcWERmIdlvy75n4e2xb1f/hhKRHW/9gQBe18xeR8tXrevtBpXsXD6jaR0Sk\nPJJ/amzw2u0p387RvEREylF5JP/KfSCa2Jb8EzE6sk57JlfiwERESmOXN3zNbIa7L8/PHwtcBkwi\naP3TtRvg7j491Cj7ygxqxm5X5w9BFw/JeHR37xQRGZJ219rnejP7gbv/FbgWuBBYDAzO4nLN+O3q\n/CEY0GV0KlHKqERESmJ31T7vA96dn9/i7vfn2/pv6DkVIc6+S3Uv+atbZxEpb7ss+ed72vxefnGB\nmf0EuB1o77bP4nDD60c14+G1vwPbSv56yldEylWhI3m9I//a/aEsB47v33BCVDMW2rdAumW7On8R\nkXJUaH/+c3uuM7O+dOxWfDXjg9emNaQSQdPPRg3oIiJlqqCmnmZ2ao/lGcCCMAIKTbe2/imV/EWk\nzBXazv/LZvZJADO7GLgfuDK0qMLQWfJvXKM6fxEpe4Um/48AZ5rZs8ChQL273xVeWCHo1r9PIhYh\nHjWV/EWkbBWa/D8E/AEYAbwGHG9mHwstqjBU7gPRCmhag5mpfx8RKWuFtvb5SP71IWB6fnKCpp+D\ng9l2wzl29u8jIlKOCm3tc07PdWY2rv/DCVm3Lh5Sibjq/EWkbPWlY7cH+i2KYqkZB01rg9mE+vQX\nkfJVUMnfzJYQVPNA0JlbDXB3WEGFJjUOXn84mE3GWNfYVuKARERKo9A6/5O7zTuw0d2bQognXDXj\noG0LdLSSSsR4rUHVPiJSngpN/pXA54ExwP3ufmN4IYWoW3PPGt3wFZEyVmid/4+BJ4FbgX8ys/lm\nNia8sELSNZzjWlLJmG74ikjZKrS1zyndFv+Uf8r3CWBaKFGFJdVZ8l/N6OoxtGdybG5JM6KqorRx\niYgUWaE3fCuBucAH8lMGuDPEuMLR1cXDWmaMqQZgeUMzR0xR8heR8lJotc/rwDnAEuAEdz/I3S8O\nL6yQVI2ESBwaVzOjNgXA8obBd99aRKSvCr3hO9Hds6FGUgxmXW396/apoiIaUfIXkbJUaMn/wu4L\nZjbCzP4zhHjClxoLjauJRoxpo6tZvk7JX0TKT6HJ/wwAM/s5gLtvBo4MK6hQ1YyDxuAp333HpFje\n0FzigEREiq/Q5F9tZlHgLDOLmVmEoO3/4FMzDhpXAzCjtpqVG1tozwz+Gi0Rkb1RaPJfTHCz92/A\nPQT9+iwMK6hQpcZB22boaGPGmBTZnLNyQ0upoxIRKapCb/h+FjgAWJp/nQH8OaSYwtX1oNcaZtTu\nA8CydU3sN7amhEGJiBRXoSX/OqAJmAQ0Ay8AN4UVVKi6unhYy/Tazrb+uukrIuWl0JL/vQQduhkQ\nB6YAt4UVVKhqtj3lWzU5xsQRlbrpKyJlp9DuHWZ1XzazKcCv+3ry/E3kRcBb7n7ynvbvF6lt/fsA\nTK+tVslfRMpOrwZzcfc3gOH9cP6vEtxHKJ6qURCJdY3oNaM2xfJ1Tbj7Ht4oIjJ0FNq3z9e6LwLv\nADZ3rnf3K/f2xGZWB3wYuAL42h527z+RSP5Br3zyH5OiOZ1lzdY2xg8fnK1XRUT2VqF1/j2bwizd\nxfq98XPgkt0dw8zOA84DmDx5ch9O1UPNOGjqLPnnb/qua1byF5GyUWid//d7rjOzw919cW9OamYn\nA+vc/SkzO243570WuBagvr6+/+plUuNg0+tA8JQvBC1+3r3f6H47hYjIQNarOn8ziwO/78N53wWc\nYmYrgJuB482sL8fbOzXjuqp9alMJapIx3fQVkbJSUPI3s0Yz29o5ASuBG3p7Une/1N3r3H0qQb9B\nD7n7p3t7vL1WMw5aN0KmHTNjRm2KZergTUTKSKHVPkPr8dfU2OC1aS2MmMyM2hQLlzWUNiYRkSIq\ntOR/bI/lEWb2uf4IwN0XFK2Nf6euEb2Cqp99x6RYu7WdxraOooYhIlIqhdb5/wjAzD4NXV06X7jb\ndwxkNfmSf+P2LX5e05O+IlImCk3+nYPcXtGL9w48PUr+M/ItflTvLyLlotB2/s1m9mug0cx+DLQC\n68ILK2RVo8GiXW39J4+sIhYxtfgRkbJRaOn9E8DTwPuA54Fq4Oywggpdj6d849EIU0ZVKfmLSNko\ntOS/xt3/Iz//WwAzOyickIqkZlvyBw3pKCLlpdCS/035oRsxs4iZfQe4I7ywiqBm/HbJf0ZtihXr\nm+nI5koYlIhIcRSa/B8GbjezeuAJgsFd6kOLqhhSY7vq/CFI/pmcs3KjhnQUkaGv0Ie8/sPMGgnG\n7f2Yu98XblhFMLwOWjZAy0aoGtnV4mf5uiZm1KZKHJyISLgKfcjrbuA0YCPwIzP7k5n9KdTIwjb1\nPcHr638H6Dako+r9RWToK/SG77+HGkUpTDwCEsNh+UNw8D8xLBln7LCE2vqLSFkotNrn7z3X5fva\n32H9oBGNwfT3wrKHwB3yHbypuaeIlINCq30O7zEdD1wUcmzhm3E8bF0F618NFvPJX0M6ishQV2i1\nz0+7zTuwHvhK/4dTZDOOD16XPwi1+zOjtprGtgwNTe2MqUmWNjYRkRAVmvxPcveh1+XlPlNh5Iyg\n3v/oL7LvmKDn6uXrmpX8RWRIK7Sd/xM9V5jZ4f0cS2nMOB5WLIRMOzPGBC1+lqneX0SGuEKTv223\nYJagb8M4Dhz7ngAdLbDyccYNS1JVEWW5WvyIyBBXaPKf1WMYx9eA60KMq3imvhsiMVj+UNeQjmrx\nIyJDXaHJf4m7D+s2TXT3odH2P1EDk44ObvoSDOyiQV1EZKgbvAOy9KcZc2HNEmhax75jUry1uZXm\n9kypoxIRCU2hyf+dAGYWDzGW0uls8vnagq5+fZau3lrCgEREwlVo8r/MzFYAq8zsATM7OMSYim/8\nbKgcCcse5Jh9R5OMR7ht8apSRyUiEppCk/9TwHR3HwtcTdC//yfDC6vIIpGg6mf5QwxPxjjlsAnc\n9czbbG0beo82iIhAgcnf3W9191x+QJeNBH36DI2mnp1mnADN62Dt85x51BRa0lnuevqtUkclIhKK\nQvv2OdfMbgHeBr4PvAkMjYe8Os2YG7wuf4jD6oZz8IRh3PjESvXzIyJDUqHVPnOA3wAz3P197v5j\nd382xLiKb9gEqD2wq73/p46awktrGlm8cnOpIxMR6XeFVvtc4O73uHszgJm928x+EW5oJbDvCfDG\nY5Bu4ZTZE0glYtz4xBuljkpEpN8V3M7fzOaY2U/yrX7+FXgptKhKZcZcyLbDG4+SSsQ4dc4E7nlu\nNZtb0qWOTESkX+02+ZvZ/mZ2mZm9BPw/YCVg7j7X3f9fUSIspinvgmii62nfM4+cQjqT49an1OxT\nRIaWPZX8XwKOB05293fnE342/LBKJF4JU44JungGDpowjMMnj+APT+rGr4gMLXtK/h8DVgPzzezX\nZnYCPXr4HHJmHA8NL8GmoK7/zKOm8FpDM4+/trHEgYmI9J/dJn93v9PdzwAOAOYTDN04xsx+aWYn\nFSPAojvooxCtgPlXAHDyoeMZltSNXxEZWgpt7dPs7n9w948AdcDTwDdDjaxU9pkCx1wIz82DNx4l\nGY/y8SMmcf8La1jf1F7q6ERE+sVe9+rp7pvc/Vp3P6G3JzWzSWY238xeNLMXzOyrvT1WKN7zNRhW\nB/f9M2QznHnUZDqyzi2LdONXRIaGUnXpnAG+7u4HAUcDXzazg0oUy44qquED/xfWPg+LrmPfMSmO\nnj6SPzz5BrmcbvyKyOBXkuTv7qvdfXF+vhFYCkwsRSy7dOApMP04eOgH0NTAp46awpsbW5n/8rpS\nRyYi0mclH8zFzKYSdB+xs0HizzOzRWa2qKGhodiBwQd/Ah3N8ODlvP/gcUwdVcV373yeLS3q7VNE\nBreSJn8zSwG3ARe5+w6jp+TvLdS7e31tbW3xA6zdH47+Ejz9eypWP8XPz5jDusZ2vn3nErX7F5FB\nrWTJPz8q2G3Aje5+e6ni2KNjL4Ga8XDfN5g9sYaLT9yfe59bzW2L1d2ziAxeJUn+ZmbAfwNL3f3K\nUsRQsEQNnPQDWP0MLP4N5x87g6OmjeSyu55nxXoN9C4ig1OpSv7vAj4DHG9mz+SnD5Uolj075H/B\nlHfDg/9CtG0TPzt9NtGI8dV5z9CRzZU6OhGRvVaq1j4L3d3c/VB3n52f7itFLAUxgw/9BNq2wh3n\nM2FYBf/2sUN59s3NXPW3V0sdnYjIXit5a59BY+xB8KEfw6v3wwPf48OHjufjR9TxiwXLeOK1DaWO\nTkRkryj57413fB6OPA8euwYW/5bLTzmYySOr+Nofn2VLq5p/isjgoeS/t97/b0HPn/d8jdTqx7nq\njDms3drGBTc9TVvH0O3tWkSGFiX/vRWNwcevh32mwrzPMLt6I1f80yE88moDX/jtIlrTugCIyMCn\n5N8blSPgzHngOfjDGZw+azg//l+HsnDZes694R+0pDOljlBEZLeU/Htr1Aw4/XewcTnccg6nzRnP\nzz4xmyde38Bnr/sHTe26AIjIwKXk3xfT3gsf/mkw5u8d/5tTDxrO1Z+cw1MrN3HWfz/B1jbdBBaR\ngUnJv6+O+Cwc/114/ja49jhOHrOBX5x5OEve2sJn/usJdQInIgOSkn9/eO834Ow/QXsj/PoEPtBy\nN78883CWrm7ktP98lKWrd+izTkSkpJT8+8u098IX/yd4ve8bvG/J1/ndmfuxsbmDU65ZyC8XLCer\ngWBEZIBQ8u9P1aPhzD/CSVfAK/dz1P0f5aHT4rzvwLH86C8vcca1j7FyQ0upoxQRUfLvd5EIHPMV\n+Nz9EI0x7KaP8h/Dfss1/zSFl9Y08sGrHubmJ1dqPAARKSkl/7BMPALOXwjv/DL29O84+e8f5eGT\n1nBY3XC+dfsSzrnhHzy9cpiWpiUAAA+WSURBVFOpoxSRMmWDpQRaX1/vixYtKnUYvbNmCdxzMaz6\nBz7lXdw+4etc/liGxrYMcyaP4HPvnsYHDh5HLKprsYj0LzN7yt3rd1iv5F8kuRw8/Vt44DJIN5M+\n4vM86PVc+WINr27MMHFEJWcfM4XT3zGZ4ZXxUkcrIkOEkv9A0dQAD3wPnr0JcDyaYPPIQ5nfui+3\nbZzK0ugBHLFfHcfuX8ux+9cyaWRVqSMWkUFMyX+gadkIKx+HN/4nmFY/C54jS5SVNoHnMpNYmpvM\npmEzGbPv4Rxx8IHMmTJSvwpEZK8o+Q90bVvhzSfhzcfxNc+Reft54k3bBolf78N4NjeDlysOZtPo\nI6iYXM9+E0Yxc1wN00ZXk4xHSxi8iAxUSv6DUesmWPsC6beWsHH5IhJrFrFPywoA2j3Osz6dRbmZ\nLM1NZmtiHJlhk0gMH8fYEdWMH55k7LAEo1MJRqUSjKquYHQqQWVFES4S7pDLQFS/UkRKTcl/qGhe\nDysfJ/vGY6Rf+x8SDUuI+LYeRDuIsYbRrMyOYj3DyRAh5xGyRMgSJRKNEovHicSSROJJohVJYhVJ\n4olK4okqMqlxtNdMJjusjoqKJIl4lEQsQnVFjOpElOpELJgqYkQjFpzUHTYsgxWPwIqFwdSyESYe\nDlOOgSnvhklHQnLYjp8nl4WmddC8DkZOh0RNkf6QIuVByX+oSrfAphWw5U3YvDJ43bKK3KaVZJvW\nk8tl8GyGXDYblMZzWfAMMe+gwtO7PGzWjdWM4s3cGN70WpqoJEuEHEYu/2qRKDOi66jnRUZ58MzC\npugollXNpqViNFNbX6CuZSlRsuSIsK56fxpSM6nMNjEsvY7q9Doq29cT8WAAHLcIzfscSPO4I2mf\ncBTZuqOJDRuLWXB9geDVcTznOE4Oy29zch5sj0aMEVVxRlTGiRnQ0QLpJuhohVgCYkmIVwavZtv/\nHTetgE2vb5vPpiExLD/VBBewRA1U1wYXq5HToWrUtuOIDDBK/rIjd8h2QLYdz7TT2txIeuNKfOPr\nsOkNbPMbxLa8QUXjSiKZlqC5qucwz2KeI0KWLbHRvFo1mxfis3g6egjLM2NpTmdp7cjSkXVi2RYO\nyb3CHH+RI1jK/raSjT6MNb4PaxjFah/JWt+HjV7DzMibHGkvMyfyKkkLekNdnhvPJmqopo0q2qiy\ndqpoo5I0EXM6PEqaGGnipInR7nFyRKi2YP9qa9/tn6CdCtLEqGH7bjdaI9VsrJhINhInkW0mkW0m\nmWsmmduxe47WSDUbExPZlJjE1uR4MhU1ZGM15CpSeEUNnqiBRA3ZSIKMxclEKshYnGz+tTK9kWFt\nb1HT9japlreobn2b6ta3MZzWyvG0VY2nrWocbVUTaK8aT0diH8yCJzQNx3CiOEQieGIYVKSIRqNE\nI0YkYsQiRsQM8ywVLWtJNK6konElFU1vYtkOiEQhEsUiMbBYsJyoxqprsepaIjW1RFNjiKVGEo1E\nMQML82KXboZNb8DmN7q9rggu4ONnB78i646EmrHhxTCEKPnLgODupLM52jM52jtytGeytGdytHVk\nSWeC9en2NhINz5Fau4gR6xcTzbaQjVaRiVWSjVaRjVWSiVWBRYl6mkg2TTQXTJFcB3iWFk/S6Aka\nc0m2ZCvYlKlgayZGIpKlytJUWQeVlqbSOkjQwabICN62sbzhY3ktU8vqdCWN7RlyDrF8Ao1FI8Qj\nTk2kjdG5TdRmVzMu8xYTsquZ4Kup8zWM9wYqrPdDeebcWMM+rPJaHGOirWcsm4jvxTGzbjRSxVav\nYgvVtJBkDJuYaOu3iy3rRgcxouQKOn7Wret4rZ6ghQStnZMnSFsF7cTpIE7aKkgTJ2Mx4uRIWIak\ndZCwDAnLUEFHcDGnjSpvpYpWqryFSm+l0lu3O2+bJVkXHUeHVTCl4zViBNWcDdGxLEscyBsV+4FF\nqaAjP6WJE/yyjeAYELHgQokZBphF8GgMInGI5F+jcSIG0Vw70Wx7/vvUTizbjpGjPT6ctop9SMdH\n0F4xkvaKEXQkRpCNVePxSjxeCbEqPF5JJBqjItNEZcvbVLWsprJ1NZWtb1PZshoM0pXjSVePJ50a\nT0fVeDKp8WQrR3ddVLe9Bn+DQyYM7/X9OiV/kWJwh0w7mdYtdLRsIdOyhUxrI9m2rVi2nUi2nUg2\njeXSWKYdy7WTS44kM2wKHcMmka6egEcqyLqTcw+qtHIZrKWBWOPbRBvfItK2KUhpFlR5uQUpLpfL\nEkk3EmnbQjS9hUj7FqLprUQ7mmlPjqalehItVXU0V9XRXD2J5uRYchYDCM6Vy+G5LJbrINLRTLR1\nA/G2jVS0B6/J9AYqOrYQy7YSz7blX1uJ54LlqG+7CMc8TSyXJkIOIPjFYxV0WDw/H6fdkrRZFa2R\nKlqsklarpMWq2GrDWBcdy9roeNZGx7E1MpwgdTuxXJqpHcvZr2MpMzuWsn/Hy9TmGrb7Jwh+BQa/\n6HJEcMhfAqAz20XIEfMsUbLEyBIj03VhbPXgItZGBe0ep53g1+QIa2IkWwu6uKc9usN+HR5ltY/E\nMcbZRhK242h/wfm2/ZLt8GA+/pk/MnW/Qwr/Hnazq+Qf69XRRGTnzCCeJBZPEhvWn9USI4D9+vF4\nRZLNQCRKzIwYkOyXgx63/WLrJrBIcA8nWkGFGRV7cTR3J5NzWjM5MrkcFokQByoMDOsqfbtDRy5H\nW3sj1rQeb9mAt2yAdDPe0YKnW6GjBe9oxTpa2JoYQUdqIulUHR2p8XRUjsEtSs6dLbks1rKeWNNq\nok2riTetJta2AculiWTbsWyaaDa4gFZn01SPHN4vf7nulPxFJDzRIqSYyn369HYzIx414gX1rRWF\n5EgYPrJP5wyMBPbvh+P0jnoSExEpQ0r+IiJlSMlfRKQMKfmLiJQhJX8RkTKk5C8iUoaU/EVEypCS\nv4hIGRo03TuYWQPwRi/fPhpY34/h9IeBGBMMzLgUU2EGYkwwMOMqp5imuHttz5WDJvn3hZkt2lnf\nFqU0EGOCgRmXYirMQIwJBmZciknVPiIiZUnJX0SkDJVL8r+21AHsxECMCQZmXIqpMAMxJhiYcZV9\nTGVR5y8iItsrl5K/iIh0o+QvIlKGhnzyN7MPmNnLZrbMzL5V6ngAzGyEmd1qZi+Z2VIze2cJYrjO\nzNaZ2fPd1v0kH9NzZnaHmY0YIHHNNrPHzewZM1tkZkcWOaZJZjbfzF40sxfM7Ks9tn/dzNzMRhcx\npqSZPWlmz+Zj+n5+/TQzeyL/fZ9nZnszqFVYMZmZXWFmr+S/7xcWK6ZusUXN7Gkzuye/fGM+Lzyf\n/87FB0BMJ5jZ4vz3fKGZ7RtqAO4+ZCcgCiwHpgMVwLPAQQMgrt8An8/PVwAjShDDe4HDgee7rTsJ\niOXnfwT8aIDE9Vfgg/n5DwELihzTeODw/HwN8Ern9wiYBNxP8ADi6CLGZEAqPx8HngCOBv4InJFf\n/yvgiwMgpnOA3wKR/LYxJfhefQ34A3BPt++R5aebivl32k1MrwAH5ue/BNwQ5vmHesn/SGCZu7/m\n7mngZuCjpQzIzIYTJLj/BnD3tLtvLnYc7v4wsLHHur+6e+eo0o8DdQMhLoJxt4fl54cDbxc5ptXu\nvjg/3wgsBSbmN/8MuIRtY4MXKyZ396b8Yjw/OXA8cGt+/W+AUwdATF8E/sXdc/n91hUrJgAzqwM+\nDPxXt1jvy8frwJMU+bu+s5go8vd8qCf/icCb3ZZXse0/balMAxqA6/M/+f7LzKpLHNPOnAv8udRB\n5F0E/MTM3gT+Hbi0VIGY2VRgDvCEmX0UeMvdny1RLFEzewZYBzxA8Ct3c7cLeNG/7z1jcvcngBnA\n6fkquz+bWbFHov85wQU613NDvrrnM8BfBkBMnwfuM7NV+Zh+GGYAQz35D0QxgmqNX7r7HKAZGBD3\nIjqZ2XeADHBjqWPJ+yJwsbtPAi4m/6up2MwsBdxGcDHKAN8GvleKWADcPevuswlKrUcCB5Qqlk49\nYzKzQ4AE0OZB1wW/Bq4rVjxmdjKwzt2f2sUu/wE87O6PDICYLgY+5O51wPXAlWHGMdST/1sEdbKd\n6vLrSmkVsCpfIoLgJ/rhJYxnO2b2WeBk4FP5n8QDwdnA7fn5WwgSXVHlS4i3ATe6++0EpdlpwLNm\ntoLgu7XYzMYVO7Z8teF84J3ACDOL5TeV7PveLaYPEHznO//97gAOLWIo7wJOyf8b3Qwcb2a/BzCz\ny4Bagrr3YtpZTPcCh3XLC/OAY8IMYqgn/38A++VbQFQAZwB/KmVA7r4GeNPMZuZXnQC8WMKQupjZ\nBwh+ip7i7i2ljqebt4Fj8/PHA68W8+RmZgS/Npa6+5UA7r7E3ce4+1R3n0qQ4A7P//sWI6baztZY\nZlYJnEhwL2I+8PH8bmcDdxUjnt3E9BJwJzA3v9uxBDc2i8LdL3X3uvy/0RnAQ+7+aTP7PPB+4JOd\n9yJKGRPBvcjhZrZ/frfOf8/QxPa8y+Dl7hkz+wpBa4wocJ27v1DisAAuAG7MX5BeI2gNUVRmdhNw\nHDA6X8d4GUFdegJ4IMh3PO7u5w+AuL4AXJUv0bYB5xUzJoKS2meAJfn6bIBvu/t9RY6ju/HAb8ws\nSlCI+6O732NmLwI3m9kPgKcpbhXZrmJaSPB9vxhoIqjbLrVfEbTQeiz/Xb/d3f+lVMHkc9UXgNvM\nLAdsIrjvFhp17yAiUoaGerWPiIjshJK/iEgZUvIXESlDSv4iImVIyV9EpAwp+YsUmZm928zuzD8/\nIFISauopkmdmWWBJt1U3u3u/9q9iZlUET29+yt239uexRfaGkr9Inpk1uXuq1HGIFIOqfUR2w8zO\nMrOm/MAfz5hZq5ndkd92Qr5n1iX5AUESZnZ2foCQQ/P7XG1ml5rZ1PxAJv+dH+jkwQHam6uUCZX8\nRfJ2Uu3zb+4+z8wWAN9w90UWjDB2MrCGoI+hE9z9FTP7LbDY3X9uZicQDMZxMbAQOJCgA7FlQL27\nP2NmNwN/dvffFO0DinSjkr/INq3uPrvbNG83+84EXnf3zk7KfkMwSA/u/mB++5XAle7enN/ndXfv\n7BtoMTC5/z+CSGGU/EXCcQvBEIa/6rauvdt8jqCzQZGSUPIX2TsZgnGXXwamdhtk+zPA37vtdwDB\nuA1tRY5PpCBK/iLbVOZv6nZOO2vmuRC4O5/UzwFuMbMlBCX5XwHkR6+qB3LdLg4iA4pu+Ir0s3xr\noDuAJDDT3b++k32+AaTc/fIihycCqOQv0q/M7AiCm703Ar8HDjOz60sblciOVPIXESlDKvmLiJQh\nJX8RkTKk5C8iUoaU/EVEypCSv4hIGfr/LZDrdtLqavIAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPhb-QgMEzBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def crossValidation(X,Y,folds): \n",
        "  seed = 1\n",
        "  np.random.seed(seed)\n",
        "  kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=seed)\n",
        "  cvscores = []\n",
        "  for train, test in kfold.split(X, Y):\n",
        "    # create model\n",
        "    model=regl2([],[],[],[],0.009)###REMEMBER TO COMMENT OUT AND BRING BACK ONLY THE MODEL\n",
        "    # Fit the model\n",
        "    model.fit(X[train], Y[train], epochs=epochs, batch_size=1000, verbose=2)\n",
        "    # evaluate the model\n",
        "    scores = model.evaluate(X[test], Y[test], verbose=0)\n",
        "    print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
        "    cvscores.append(scores[1] * 100)\n",
        "  print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-RvjiZLDupt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def grid(model_function,X,Y):  # https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ \n",
        "  \n",
        "  dense_layer_sizes = [(128,64,32,16),(128,128,128),(256,256,128),(512,1024)]\n",
        "  param_grid = dict(dense_layer_sizes=dense_layer_sizes)\n",
        "  print(param_grid)\n",
        "  model = KerasClassifier(build_fn=model_function,epochs=50, batch_size=1000)\n",
        "  grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1 ,cv=2,verbose=1)\n",
        "  grid_result = grid.fit(X, Y)\n",
        "  # summarize results\n",
        "  print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "  means = grid_result.cv_results_['mean_test_score']\n",
        "  stds = grid_result.cv_results_['std_test_score']\n",
        "  params = grid_result.cv_results_['params']\n",
        "  for mean, stdev, param in zip(means, stds, params):\n",
        "      print(\"%f (%f) with: %r\" % (mean, stdev, param))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHawxTTqXXJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "40ced8ac-082b-4534-fc7b-2e83f89fee1d"
      },
      "source": [
        "grid(create_model,X,Y)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'dense_layer_sizes': [(128, 64, 32, 16), (128, 128, 128), (256, 256, 128), (512, 1024)]}\n",
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 10.2min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 4s 72us/step - loss: 0.5581 - accuracy: 0.7153\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.3318 - accuracy: 0.8610\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.2947 - accuracy: 0.8746\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.2614 - accuracy: 0.8895\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.2268 - accuracy: 0.9056\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.1918 - accuracy: 0.9212\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.1578 - accuracy: 0.9364\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.1276 - accuracy: 0.9484\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0995 - accuracy: 0.9612\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0824 - accuracy: 0.9676\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0700 - accuracy: 0.9733\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 3s 61us/step - loss: 0.0598 - accuracy: 0.9779\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0493 - accuracy: 0.9816\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0417 - accuracy: 0.9846\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0360 - accuracy: 0.9865\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0420 - accuracy: 0.9856\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0330 - accuracy: 0.9887\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0249 - accuracy: 0.9914\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0244 - accuracy: 0.9918\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0276 - accuracy: 0.9907\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0255 - accuracy: 0.9914\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0282 - accuracy: 0.9901\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0185 - accuracy: 0.9943\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0165 - accuracy: 0.9947\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0165 - accuracy: 0.9946\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0161 - accuracy: 0.9942\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0185 - accuracy: 0.9936\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0155 - accuracy: 0.9951\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0158 - accuracy: 0.9946\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0181 - accuracy: 0.9939\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 3s 58us/step - loss: 0.0142 - accuracy: 0.9956\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0125 - accuracy: 0.9956\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0142 - accuracy: 0.9951\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0140 - accuracy: 0.9952\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0116 - accuracy: 0.9960\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0140 - accuracy: 0.9953\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0106 - accuracy: 0.9967\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0125 - accuracy: 0.9958\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0107 - accuracy: 0.9966\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0104 - accuracy: 0.9964\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0122 - accuracy: 0.9957\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0095 - accuracy: 0.9967\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0099 - accuracy: 0.9968\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0103 - accuracy: 0.9968\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0092 - accuracy: 0.9969\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0104 - accuracy: 0.9967\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0081 - accuracy: 0.9974\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0114 - accuracy: 0.9962\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 3s 59us/step - loss: 0.0082 - accuracy: 0.9974\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 3s 60us/step - loss: 0.0088 - accuracy: 0.9972\n",
            "Best: 0.850000 using {'dense_layer_sizes': (256, 256, 128)}\n",
            "0.849600 (0.001760) with: {'dense_layer_sizes': (128, 64, 32, 16)}\n",
            "0.848580 (0.000820) with: {'dense_layer_sizes': (128, 128, 128)}\n",
            "0.850000 (0.000240) with: {'dense_layer_sizes': (256, 256, 128)}\n",
            "0.847080 (0.000320) with: {'dense_layer_sizes': (512, 1024)}\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
